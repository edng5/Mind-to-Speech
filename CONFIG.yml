# MLP Hidden Layer Configuration
# hidden_units_1: 4096
# hidden_units_2: 2048
# hidden_units_3: 1024
# hidden_units_4: 512
# hidden_units_5: 256
# hidden_units_6: 128
# hidden_units_7: 64
dropout_rate: 0.3
learning_rate: 0.0001
batch_size: 64
epochs: 100
train_split: 0.8
save_threshold: 0.65